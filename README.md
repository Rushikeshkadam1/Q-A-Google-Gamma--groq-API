# Gemma Model Document Q&A

## Aim

The objective of this project is to develop an interactive question-answering system that leverages a document store to provide precise answers based on the context within a given set of documents. The system integrates with various language models, including Groq's Llama3 model and Google's Generative AI, to process and retrieve information from large PDF collections. The application is built with a user-friendly interface using Streamlit, which allows users to upload documents and ask questions related to the content.

## Methodology

The system follows a multi-step process:

1. **Document Ingestion**: The system loads documents (PDF files) from a specified directory using `PyPDFDirectoryLoader`. The documents are then split into manageable chunks using the `RecursiveCharacterTextSplitter` to ensure the model can process them efficiently.

2. **Vector Embedding**: The documents are transformed into vector embeddings using Google's Generative AI Embeddings (`embedding-001`). These embeddings help convert textual content into numerical vectors, which are then indexed in a vector store using `FAISS`.

3. **Retrieval**: Upon user query submission, the relevant documents are retrieved from the vector store. This is done using a retriever built from the FAISS vector store, and a retrieval chain is used to process the query and retrieve contextually relevant information.

4. **Model Processing**: The Groq-powered Llama3 model processes the retrieved documents in conjunction with a custom-designed prompt to provide the most accurate answer based on the context of the documents. The model response is displayed in the Streamlit interface.

5. **User Interaction**: Users can enter their questions through a text input field, and upon pressing the "Documents Embedding" button, the vector embeddings are generated, and the system is ready to answer queries.

## Libraries Used

- **Streamlit**: For creating an interactive and user-friendly interface.
- **Groq**: For deploying the Llama3 language model to process and answer questions.
- **LangChain**: A framework used to facilitate document processing, including splitting, retrieval, and embedding.
- **FAISS**: A library for efficient similarity search and clustering of dense vectors.
- **Google Generative AI**: Used for generating document embeddings.
- **PyPDF2**: For extracting text from PDF files.
- **dotenv**: To load environment variables securely.
  
## Evaluation Metric

- **Response Time**: The time taken for the system to process a user's query and provide an answer. This metric is crucial for assessing the system's responsiveness and efficiency.
- **Answer Accuracy**: The accuracy of the provided answer in relation to the question asked and the content of the documents. This can be evaluated by manual comparison or by using automated techniques such as BLEU or ROUGE scores, although it is not explicitly calculated in the current setup.
  
## Results

The system efficiently processes document embeddings and provides accurate answers to user queries based on the context in the uploaded documents. The following steps illustrate the system's behavior:

1. **Document Ingestion**: The system loads PDF files from a user-specified directory and converts the content into chunks for easier processing.
2. **Embedding Generation**: The documents are converted into vector embeddings and stored in FAISS, making them ready for retrieval.
3. **Query Response**: When a user enters a question, the relevant document chunks are retrieved, and the Groq model generates a contextually accurate answer.
4. **Similarity Search**: The system also performs a similarity search, showing users the most relevant document chunks related to their query.

For instance, when the user enters a question about the documents, the system responds with the most relevant text extracted from the documents and the answer generated by the Llama3 model.

## Conclusion

This project demonstrates a functional document-based Q&A system using cutting-edge technologies like Groq's Llama3 model, Google's Generative AI, and FAISS for efficient document retrieval. The integration of LangChain makes it easy to orchestrate document processing and retrieval tasks, while the user-friendly Streamlit interface enhances the overall user experience. The system provides fast, contextually relevant responses to user queries, making it suitable for applications like knowledge bases, legal document analysis, and research assistance.

## Future Work

1. **Improved Model Selection**: While Groq's Llama3 model performs well, experimenting with other models or fine-tuning the current model could improve the accuracy and relevance of answers.
   
2. **User Feedback**: Implement a feedback mechanism that allows users to rate the quality of responses, which can be used to improve the model and document retrieval process.

3. **Document Preprocessing**: Enhance the preprocessing pipeline to handle more complex document formats, such as scanned PDFs, by integrating OCR (Optical Character Recognition) tools.

4. **Scalability**: Extend the system to handle larger datasets and more complex queries by integrating distributed processing frameworks or deploying the system on cloud infrastructure.

5. **Web Deployment**: Deploy the application on a cloud service like AWS, Azure, or Google Cloud to make it accessible to a wider audience and handle higher query volumes.

6. **Multi-Language Support**: Integrate multilingual models and techniques to support queries in various languages and expand the system's global applicability.

## How to Run

### Prerequisites
- Python 3.x
- Required Python packages from `requirements.txt`

### Installation

1. Clone the repository:
   ```bash
   git clone https://github.com/username/Gemma-Model-Document-QA.git
   cd Gemma-Model-Document-QA
   ```
2. Install the dependencies:
```bash
pip install -r requirements.txt
```
3. Set up environment variables:
Create a ```.env``` file in the root directory and add the following:
```bash
GROQ_API_KEY=your_groq_api_key
GOOGLE_API_KEY=your_google_api_key
```
4. Run the Streamlit application:
```bash
streamlit run app.py
```
5. Open the provided local URL in your browser to use the app.
